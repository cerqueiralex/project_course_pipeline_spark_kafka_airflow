# Pipeline de extra√ß√£o de dados em Tempo real atrav√©s da API Random User Generator com Kafka, Spark e Airflow

Este projeto implementa um pipeline ETL utilizando Airflow e Docker para extrair dados meteorol√≥gicos de uma API p√∫blica e armazen√°-los em um banco de dados SQLite. A aplica√ß√£o realiza requisi√ß√µes peri√≥dicas para capturar informa√ß√µes como temperatura, velocidade do vento, condi√ß√£o do tempo, umidade e visibilidade, estruturando e salvando os dados em formato CSV para posterior an√°lise. O ambiente √© orquestrado com Docker Compose, e a configura√ß√£o segue os padr√µes recomendados pelo Apache Airflow, garantindo f√°cil reprodutibilidade e escalabilidade do fluxo de dados.

<img src="https://i.imgur.com/bHA1YzQ.png" style="width:100%;height:auto"/>

Tecnologias:  
* Apache Airflow
* Apache Kafka
* Apache Spark
* Confluent Control Center
* Schema Registry for Confluent Platform
* Docker
* DB Cassandra
* Python

## API Random User Generator

A free, open-source API for generating random user data. Like Lorem Ipsum, but for people.

https://randomuser.me/

## Configura√ß√µes da Stack Docker 

### üñ•Ô∏è 01. M√°quina Servidor

Documenta√ß√£o: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

Acessando pasta do projeto com cd/path

```
docker compose up -d
```

Imagens Docker (Microservi√ßos):

* Apache Kafka (image: bitnami/kafka:3.9.0)  
* Confluent Schema Registry (image: confluentinc/cp-schema-registry:7.4.0)  
* Confluent Control Center (image: confluentinc/cp-enterprise-control-center:7.4.0)  
* Apache Airflow Webserver (apache/airflow:2.10.4-python3.11)  
* Apache Airflow Scheduler (image: apache/airflow:2.10.4-python3.11)
* Banco de dados PostgreSQL usado pelo Airflow (image: postgres:14.0)
* Cluster Spark
  * spark-master: image: bitnami/spark:3.5.4
  * spark-worker-1: image: bitnami/spark:3.5.4
* Cassandra DB (image: cassandra:5.0.2)

### üñ•Ô∏è 02. M√°quina Cliente

**Dockerfile**

* Linux com Interpretador Python (python:3.11-slim)
* Depend√™ncias:
  * pyspark==3.5.4
  * kafka-python==2.0.2
  * cassandra-driver==3.29.2

Acessando pasta do projeto com cd/path

01. Criar imagem Cassandra
```
docker build -t kafka-spark-cassandra-consumer .
```
02. Criar Container
```
docker run --name dsa_cliente -dit --network dsaservidor_dsacademy kafka-spark-cassandra-consumer
```

Dentro do container, o comando abaixo inicia o Kafka Consumer (conecta ao spark, kafka e cassandra):
```
python dsa_consumer_stream.py --mode initial
```


> Nota: No final do Dockerfile ao descomentar a √∫ltima linha e recriar imagem e container, n√£o ser√° necess√°rio a ativa√ß√£o manual da dag e o container ir√° processar os dados automaticamente.

```
# Comando de entrada padr√£o
#CMD ["python", "dsa_consumer_stream.py", "--mode", "append"]
```


## Pipeline Scripts
```
dsa_consumer_stream.py
```

Esse c√≥digo implementa um pipeline de ingest√£o e processamento de dados em tempo real com Apache Spark, Kafka e Cassandra.  

Primeiro, ele configura logging, importa bibliotecas necess√°rias e define fun√ß√µes auxiliares para criar o keyspace e a tabela no Cassandra, al√©m de formatar e inserir dados.  

Em seguida, o programa principal aceita um argumento de linha de comando (--mode) para definir se os dados do Kafka devem ser lidos desde o in√≠cio ou somente os novos.  

Ele cria conex√µes com o Cassandra e Spark, configura a leitura de um t√≥pico Kafka, e define um schema para os dados JSON consumidos.  

Esses dados s√£o transformados em um DataFrame estruturado, filtrados por e-mails v√°lidos, e ent√£o inseridos no Cassandra lote a lote com foreachBatch, mantendo o processamento cont√≠nuo at√© ser encerrado. 

```
dsa_kafka_stream.py
```

Esse c√≥digo define uma DAG no Apache Airflow chamada dsa-real-time-etl-stack, que agenda diariamente a execu√ß√£o de uma tarefa Python (streaming_task) respons√°vel por fazer streaming em tempo real de dados de usu√°rios aleat√≥rios obtidos via API p√∫blica. 

O processo come√ßa com a fun√ß√£o dsa_extrai_dados_api(), que consome dados da API RandomUser.me, seguida da fun√ß√£o dsa_formata_dados(), que transforma os dados brutos em um formato estruturado com campos como nome, endere√ßo, e-mail e imagem de perfil, atribuindo tamb√©m um UUID √∫nico para cada usu√°rio. 

Em seguida, a fun√ß√£o dsa_stream_dados() estabelece uma conex√£o com um broker Kafka (em broker:29092), e por 60 segundos envia os dados formatados para o t√≥pico dsa_kafka_topic, registrando logs de sucesso ou erro. 

Todo esse fluxo √© encapsulado em uma DAG do Airflow, permitindo a orquestra√ß√£o e agendamento automatizado da pipeline de ETL em tempo real.


## Resultado

Acessar o Cassandra e verificar o resultado do armazenamento:  

Esse conjunto de comandos serve para acessar o banco de dados Cassandra em um container Docker e consultar dados de uma tabela.  

Abre o terminal interativo do Cassandra (CQLSH) dentro do container chamado cassandra.
```
docker exec -it cassandra cqlsh
```
Define o keyspace (similar ao schema em bancos relacionais) chamado dsa_dados_usuarios como o contexto atual para execu√ß√£o dos pr√≥ximos comandos:
```
USE dsa_dados_usuarios;
```
Realiza uma consulta SQL/CQL para retornar todos os registros da tabela tb_usuarios dentro do keyspace selecionado.
```
SELECT * FROM tb_usuarios;
```
<img src="https://i.imgur.com/3mywWWV.png" style="width:100%;height:auto"/>

